{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import nn, optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 60000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Example: Display the size of the training set\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels (before noise): [5, 0, 4, 1, 9, 2, 1, 3, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Original labels (before noise):\", train_dataset.targets[:10].tolist())\n",
    "\n",
    "# Function to add label noise by shuffling labels\n",
    "def add_label_noise(dataset, noise_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Adds label noise by shuffling labels for a fraction of the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: A PyTorch dataset object (e.g., MNIST).\n",
    "        noise_fraction: Fraction of data to corrupt (0 <= noise_fraction <= 1).\n",
    "\n",
    "    Returns:\n",
    "        The dataset with shuffled labels for the specified fraction.\n",
    "    \"\"\"\n",
    "    # Total number of samples\n",
    "    num_samples = len(dataset)\n",
    "    \n",
    "    # Number of labels to corrupt\n",
    "    num_noisy_labels = int(noise_fraction * num_samples)\n",
    "    \n",
    "    # Get the indices to shuffle\n",
    "    indices = np.random.choice(num_samples, num_noisy_labels, replace=False)\n",
    "    \n",
    "    # Shuffle the labels\n",
    "    noisy_targets = dataset.targets.clone()  # Clone the targets to avoid modifying the original directly\n",
    "    noisy_targets[indices] = torch.randint(0, 10, size=(num_noisy_labels,))  # Random labels in the range [0, 9]\n",
    "    \n",
    "    # Update the dataset's targets\n",
    "    dataset.targets = noisy_targets\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m train_subset \u001b[38;5;241m=\u001b[39m Subset(train_dataset, indices)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Add noise to the training dataset (e.g., 20% label noise)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_subset \u001b[38;5;241m=\u001b[39m \u001b[43madd_label_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36madd_label_noise\u001b[0;34m(dataset, noise_fraction)\u001b[0m\n\u001b[1;32m     22\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(num_samples, num_noisy_labels, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Shuffle the labels\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m noisy_targets \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# Clone the targets to avoid modifying the original directly\u001b[39;00m\n\u001b[1;32m     26\u001b[0m noisy_targets[indices] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, size\u001b[38;5;241m=\u001b[39m(num_noisy_labels,))  \u001b[38;5;66;03m# Random labels in the range [0, 9]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Update the dataset's targets\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'targets'"
     ]
    }
   ],
   "source": [
    "subset_size = 10000  # Use only 10,000 samples\n",
    "indices = np.random.choice(len(train_dataset), subset_size, replace=False)  # Randomly select indices\n",
    "train_subset = Subset(train_dataset, indices)\n",
    "# Add noise to the training dataset (e.g., 20% label noise)\n",
    "train_subset = add_label_noise(train_subset, noise_fraction=0.2)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with SGD...\n",
      "Epoch [1/30], Training Loss: 1.7370, Training Accuracy: 40.04%\n",
      "Epoch [2/30], Training Loss: 0.5442, Training Accuracy: 82.34%\n",
      "Epoch [3/30], Training Loss: 0.3641, Training Accuracy: 88.67%\n",
      "Epoch [4/30], Training Loss: 0.2860, Training Accuracy: 91.36%\n",
      "Epoch [5/30], Training Loss: 0.2255, Training Accuracy: 93.06%\n",
      "Epoch [6/30], Training Loss: 0.1819, Training Accuracy: 94.31%\n",
      "Epoch [7/30], Training Loss: 0.1538, Training Accuracy: 95.16%\n",
      "Epoch [8/30], Training Loss: 0.1468, Training Accuracy: 95.36%\n",
      "Epoch [9/30], Training Loss: 0.1221, Training Accuracy: 96.18%\n",
      "Epoch [10/30], Training Loss: 0.1045, Training Accuracy: 96.72%\n",
      "Epoch [11/30], Training Loss: 0.0956, Training Accuracy: 97.02%\n",
      "Epoch [12/30], Training Loss: 0.0893, Training Accuracy: 97.12%\n",
      "Epoch [13/30], Training Loss: 0.0738, Training Accuracy: 97.69%\n",
      "Epoch [14/30], Training Loss: 0.0625, Training Accuracy: 97.94%\n",
      "Epoch [15/30], Training Loss: 0.0488, Training Accuracy: 98.41%\n",
      "Epoch [16/30], Training Loss: 0.0415, Training Accuracy: 98.56%\n",
      "Epoch [17/30], Training Loss: 0.0390, Training Accuracy: 98.78%\n",
      "Epoch [18/30], Training Loss: 0.0295, Training Accuracy: 98.99%\n",
      "Epoch [19/30], Training Loss: 0.0324, Training Accuracy: 98.93%\n",
      "Epoch [20/30], Training Loss: 0.0221, Training Accuracy: 99.32%\n",
      "Epoch [21/30], Training Loss: 0.0318, Training Accuracy: 98.97%\n",
      "Epoch [22/30], Training Loss: 0.0438, Training Accuracy: 98.61%\n",
      "Epoch [23/30], Training Loss: 0.0378, Training Accuracy: 98.69%\n",
      "Epoch [24/30], Training Loss: 0.0111, Training Accuracy: 99.72%\n",
      "Epoch [25/30], Training Loss: 0.0195, Training Accuracy: 99.40%\n",
      "Epoch [26/30], Training Loss: 0.0222, Training Accuracy: 99.30%\n",
      "Epoch [27/30], Training Loss: 0.0613, Training Accuracy: 98.10%\n",
      "Epoch [28/30], Training Loss: 0.0111, Training Accuracy: 99.73%\n",
      "Epoch [29/30], Training Loss: 0.0055, Training Accuracy: 99.89%\n",
      "Epoch [30/30], Training Loss: 0.0361, Training Accuracy: 98.74%\n",
      "Test Accuracy: 95.01%\n",
      "\n",
      "Training with Adam...\n",
      "Epoch [1/30], Training Loss: 0.8801, Training Accuracy: 71.09%\n",
      "Epoch [2/30], Training Loss: 0.3703, Training Accuracy: 88.61%\n",
      "Epoch [3/30], Training Loss: 0.2879, Training Accuracy: 91.25%\n",
      "Epoch [4/30], Training Loss: 0.2219, Training Accuracy: 92.90%\n",
      "Epoch [5/30], Training Loss: 0.1825, Training Accuracy: 94.50%\n",
      "Epoch [6/30], Training Loss: 0.1575, Training Accuracy: 94.88%\n",
      "Epoch [7/30], Training Loss: 0.1305, Training Accuracy: 95.97%\n",
      "Epoch [8/30], Training Loss: 0.1169, Training Accuracy: 96.29%\n",
      "Epoch [9/30], Training Loss: 0.1080, Training Accuracy: 96.62%\n",
      "Epoch [10/30], Training Loss: 0.0906, Training Accuracy: 97.15%\n",
      "Epoch [11/30], Training Loss: 0.0895, Training Accuracy: 97.06%\n",
      "Epoch [12/30], Training Loss: 0.0728, Training Accuracy: 97.65%\n",
      "Epoch [13/30], Training Loss: 0.0689, Training Accuracy: 97.71%\n",
      "Epoch [14/30], Training Loss: 0.0537, Training Accuracy: 98.32%\n",
      "Epoch [15/30], Training Loss: 0.0477, Training Accuracy: 98.35%\n",
      "Epoch [16/30], Training Loss: 0.0636, Training Accuracy: 97.55%\n",
      "Epoch [17/30], Training Loss: 0.0540, Training Accuracy: 98.22%\n",
      "Epoch [18/30], Training Loss: 0.0301, Training Accuracy: 99.06%\n",
      "Epoch [19/30], Training Loss: 0.0470, Training Accuracy: 98.42%\n",
      "Epoch [20/30], Training Loss: 0.0357, Training Accuracy: 98.92%\n",
      "Epoch [21/30], Training Loss: 0.0371, Training Accuracy: 98.81%\n",
      "Epoch [22/30], Training Loss: 0.0335, Training Accuracy: 98.89%\n",
      "Epoch [23/30], Training Loss: 0.0266, Training Accuracy: 99.03%\n",
      "Epoch [24/30], Training Loss: 0.0341, Training Accuracy: 98.85%\n",
      "Epoch [25/30], Training Loss: 0.0381, Training Accuracy: 98.69%\n",
      "Epoch [26/30], Training Loss: 0.0353, Training Accuracy: 98.76%\n",
      "Epoch [27/30], Training Loss: 0.0193, Training Accuracy: 99.36%\n",
      "Epoch [28/30], Training Loss: 0.0268, Training Accuracy: 99.08%\n",
      "Epoch [29/30], Training Loss: 0.0365, Training Accuracy: 99.03%\n",
      "Epoch [30/30], Training Loss: 0.0331, Training Accuracy: 99.01%\n",
      "Test Accuracy: 95.02%\n",
      "\n",
      "Training with RMSprop...\n",
      "Epoch [1/30], Training Loss: 1.1351, Training Accuracy: 60.18%\n",
      "Epoch [2/30], Training Loss: 0.3887, Training Accuracy: 87.89%\n",
      "Epoch [3/30], Training Loss: 0.2894, Training Accuracy: 91.01%\n",
      "Epoch [4/30], Training Loss: 0.2157, Training Accuracy: 93.23%\n",
      "Epoch [5/30], Training Loss: 0.1921, Training Accuracy: 94.15%\n",
      "Epoch [6/30], Training Loss: 0.1673, Training Accuracy: 94.78%\n",
      "Epoch [7/30], Training Loss: 0.1332, Training Accuracy: 95.91%\n",
      "Epoch [8/30], Training Loss: 0.1205, Training Accuracy: 96.30%\n",
      "Epoch [9/30], Training Loss: 0.1024, Training Accuracy: 96.89%\n",
      "Epoch [10/30], Training Loss: 0.0946, Training Accuracy: 97.28%\n",
      "Epoch [11/30], Training Loss: 0.0892, Training Accuracy: 97.10%\n",
      "Epoch [12/30], Training Loss: 0.0768, Training Accuracy: 97.67%\n",
      "Epoch [13/30], Training Loss: 0.0728, Training Accuracy: 97.87%\n",
      "Epoch [14/30], Training Loss: 0.0692, Training Accuracy: 97.96%\n",
      "Epoch [15/30], Training Loss: 0.0513, Training Accuracy: 98.39%\n",
      "Epoch [16/30], Training Loss: 0.0602, Training Accuracy: 98.27%\n",
      "Epoch [17/30], Training Loss: 0.0569, Training Accuracy: 98.47%\n",
      "Epoch [18/30], Training Loss: 0.0403, Training Accuracy: 98.82%\n",
      "Epoch [19/30], Training Loss: 0.0386, Training Accuracy: 98.75%\n",
      "Epoch [20/30], Training Loss: 0.0468, Training Accuracy: 98.53%\n",
      "Epoch [21/30], Training Loss: 0.0462, Training Accuracy: 98.62%\n",
      "Epoch [22/30], Training Loss: 0.0436, Training Accuracy: 98.79%\n",
      "Epoch [23/30], Training Loss: 0.0350, Training Accuracy: 99.11%\n",
      "Epoch [24/30], Training Loss: 0.0420, Training Accuracy: 99.03%\n",
      "Epoch [25/30], Training Loss: 0.0315, Training Accuracy: 99.02%\n",
      "Epoch [26/30], Training Loss: 0.0268, Training Accuracy: 99.15%\n",
      "Epoch [27/30], Training Loss: 0.0355, Training Accuracy: 99.04%\n",
      "Epoch [28/30], Training Loss: 0.0258, Training Accuracy: 99.21%\n",
      "Epoch [29/30], Training Loss: 0.0373, Training Accuracy: 98.97%\n",
      "Epoch [30/30], Training Loss: 0.0364, Training Accuracy: 98.98%\n",
      "Test Accuracy: 96.32%\n"
     ]
    }
   ],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),  # First layer with 512 neurons\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.3),       # Dropout for regularization\n",
    "            nn.Linear(512, 256),   # Second layer with 256 neurons\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),   # Third layer with 128 neurons\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),    # Fourth layer with 64 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)      # Output layer for 10 classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input\n",
    "        return self.model(x)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DeepNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizers\n",
    "optimizers = {\n",
    "    'SGD': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
    "    'Adam': optim.Adam(model.parameters(), lr=0.001),\n",
    "    'RMSprop': optim.RMSprop(model.parameters(), lr=0.001)\n",
    "}\n",
    "\n",
    "# Training function\n",
    "def train_model(optimizer_name, optimizer, num_epochs=30):\n",
    "    print(f\"\\nTraining with {optimizer_name}...\")\n",
    "    model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    evaluate_model()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Train the model with each optimizer\n",
    "for opt_name, opt in optimizers.items():\n",
    "    train_model(opt_name, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels (before noise): [5, 2, 0, 9, 4, 6, 8, 6, 1, 6]\n",
      "Labels after adding noise: [5, 9, 0, 9, 4, 9, 5, 6, 1, 6]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nntorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
